## 2.1 从 `CPU` 联系到 `Java`
首先清楚：每个进程或线程发出操作请求后，最后会由 `CPU` 来分配时间片处理，处理时将操作数传递给 `CPU` , `CPU` 计算将其写回到“本地变量”中，这个本地变量通常存在于程序所谓的“栈”中，如果多次对这些本地变量进行操作，为了提升运算效率，它们有可能会被 `Cache` 到 `CPU` 的缓存中。 `CPU` 有寄存器、一级缓存、二级缓存，有的也有三级缓存， `CPU` 为什么有这么多组件呢？其实就是一个“就近原则”。  
  
打个比方，去某个地方办“几件事”，不会办完一件事情回家一趟，再坐车去办下一件事，因为时间开销大。如果发现办完一件事情后就在庞斑可以办下另一件事，则直接继续办理：如果发现有点累，则在外面小坐休息会，而不用回家休息。  
  
`CPU` 也可以一次做几件事情，所以它会有一个小的 `Cache` 区域，将这些内容暂时 `Cache` 住，这样距离处理器越近，操作的延迟就越小，整体效率自然就越快。
>一般来讲，一级缓存与 `CPU` 的延迟一般在 2~3ns 之间，二级缓存通常为 10~15ns ， 三级缓存为 20~30ns ，而内存通常会在 50ns 以上甚至更高。  
  
### 2.1.1 初识多核
在多核的 `Cache` 中（理解为多线程中的缓存策略），对某些数据 `Cache` 后，数据在“写入”和“读取”的时候必须满足一些规范，通常叫做“缓存一致性协议”。通过这种规范来实现架构，用以满足多个 `CPU` 对同一个变量修改时，相互之间都是知道的。不过，它并不能保证所有的数据都是这样的，因为这样做的开销是巨大的，所以在某些时候是允许不一致情况发生的（在分布式里，叫做最终一致性协议）。  
  
问题：我们编写的程序如何和 `CPU` 进行交互的？它是否会被 `Cache` ？是否存在并发问题？在可能的情况下，如何利用 `CPU` 来提高程序运行效率？  
  
编写 `Java` 程序时，大部分操作都是申请对象和操作对象，而对象实例的空间大部分存储在 `Java` 的堆（ `Heap` ）中，而 `Java` 的“栈”是中存储什么呢？  
  
`Java` 的“栈”更多的是通过 `JVM` 与 `OS` 一起管理的一块区域，写过 `C/C++` 程序的人应该知道，如果“不是”通过 `malloc` 、 `realloc` 、 `new` 等关键字申请的内存空间，那么作用域结束时自然释放，因为这部分的回收操作是由 `OS` 来管理和控制的。  
  
在 `Java` 程序中“栈空间”也是类似的，当程序的局部变量中使用基本类型时，它直接在“栈”上申请了一些空间，而当使用引用来引用对象时，这些引用的空间也位于“栈”上。（实际对象在堆中）。
>确切地说，在编译阶段， `Java` 就可以决定方法的“本地变量”（ `LocalVariable` ）的个数，因此在方法调用的时候，就可以直接分配一个本地变量的区域。这个空间是基于 `slot` 来分配的，每个 `slot` 占用 `32bit` ，就算是 `boolean` 也会占用这么宽，当然 `long` 、 `double` 会占用 2 个 `slot` 。这些 `slot` 可以被复用，也就是说，在方法体内部，如果某个局部变量是在循环或判定语句内部声明的，那么在退出这个区域后，对应的 `slot` 是可以被释放给它之后声明的局部变量使用的。

在程序运行过程中，会通过 `Java` 的虚指令来完成对 `Java` 虚拟机中对象和数据做一些操作。虚指令只是 `Java` 的指令，而不是最终指令，有它才会跨平台，它最终会在对应 `OS` 的虚拟机上被翻译为汇编指令来完成实际硬件的运行操作。

### 2.2.2 虚指令
源码：
```java
public static void main(String[] args) {
  int a = 1;
  int b = 2;
  int c = a + b;
}
```
将源码编译为 `class` 文件后，在字节码文件目录下使用： `javap -verbose .\Test.class` ，即可查看字节码文件。其中 `-verbose` 选项为输出常量信息。  
  
`JVM` 发出指令请求，由 `OS` 去完成具体工作， `JVM` 自身无法做计算工作。 `JVM` 内部对象的操作，例如字符串叠加，表面上也只是调用了一些数组拷贝。**也就是说， `JVM` 只关注自己的事情并组好，将其它的职责交给别人完成**。  
  
## 2.2 多核
多核 `CPU` 就像有多个计算机中心做事情。多个 `CPU` 要发挥最大效应就是不让某些人偷懒，自然就需要一些算法来协调和管理，让请求负载均衡（处处都有负载均衡啊）。比方说这个指令由哪个 `CPU` 来处理呢？同一份数据被多个 `CPU` 处理，并对其修改后，如何让其它的 `CPU` 知道呢？这中间产生各种各样的问题，算法也层出不穷，为了解决一个问题而将问题变得更加复杂，但是为了发展又不得不去做。在工作中也同样遇到系统不断变得复杂的过程，这是社会需求的进步，即使计算机部件也是同样的命运。  
  
当用户发起一个计算请求的时候，例如一个中断，这么多的 `CPU` 会干什么呢？这就要从人物的模型开始说起。起初，操作系统可能是不断地定时扫描各个部件看是否有指令来，有的话就处理，但是显然这种模式会很慢，因为“知道的时候往往都晚了”，这就是所谓的“非实时”。  
  
后来， `CPU` 有了“中断”模型，通过中断来完成调用，貌似实时性很强，但是某些部件发起中断频率非常高（例如鼠标移动，就在不断地发送指令）。此时对中断会有一个缓存区，由于 `CPU` 的处理速度非常快，可以在一瞬间处理大批量的请求，所以这种缓冲区是不错的设计思路（在许多设计中都会用到）。[点我详情了解中断机制](1)
  
一系列任务可能要做各种各样的事情，在通常情况下， `CPU` 的计算速度非常快，很快就可以完成指令。但是操作系统不希望 `CPU` 为一个“等待指令”或者是长期执行的任务使得自己“陷入困境”，比如一些 `I/O` 等待（网络 `I/O` 和磁盘 `I/O`），它中途基本都不参与，而是以事件注册的方式来实现回调，对于某些执行时间长的任务， `CPU` 可能会分配一些时间片来处理其它的任务。
>当 `CPU` 不断去切换任务处理的时候，又会有一种新的现象出现，那就是“上下文切换”。

当多个 `CPU` 在同一台计算机中出现的时候，会出现什么情况呢？是大家一起去抢指令？还是由一个 `CPU` 来分配指令？抑或是某些 `CPU` 专门管理某一块区域的指令？
  
`CPU` 是一个大家都要用的资源，那么就必须要有一个规范，而如何来规范呢？这没有明确的说法，只是大家的实现方法不同、架构不同，也许在不同的测试场景下效果会不同，调优的方法也有变化。这些会体现在计算机兼容性上， `CPU` 、内存、硬盘、主板等，在熟悉了具体厂商的实现机制后，相互配合兼容性就会做得更好。  
  
现在来说“大家一起去抢指令”，这种方式是不多见的，只是在某些系统的设计中有多进程模式，让多个进程监听同一个端口，当这个端口得到信号时，可能多个进程同时被唤醒的现象还是存在的，我们通常称之为“惊群”。（在监控中很多都会有这个情况，例如ZK一个节点被许多无意义的客户端监听）  
  
而“由一个 `CPU` 来分配指令”，这种方式也存在，那这样会不会让这个 `CPU` 很忙？也就是这个 `CPU` 会不会出现相对“飙”高的现象，尤其是有大量请求的时候？没错，它也有缺陷，只是通常情况下问题不会太大，而且它至少实现了调度，在有些时候可以划分下“领土”，使资源隔离是比较“靠谱”的。（类Master/Slave机制）  
  
那么我们按照板块来划分，每个板块有一个或多个 `CPU` 来管理对应的内存区域，是不是就合理了？也未必。首先，数据还需要和其它的部件通信，其它的部件资源是不是也是隔离的？要做到完全的物理划分是十分复杂的，通常意义上的物理划分只是相对底层的指令量、数据量的协调而已；其次，板块划分完后，就完全有可能出现热点问题（即某个 `CPU` 的任务非常重），如果需要热点问题我们应当如何处理呢？这个时候我们就要想其它的方法了。  

那到底哪种方法好呢？

其实没有好坏之分，只有场景的选择。

## 2.3 Cache line
前面提到“大家办事就近原则”，也就是一次可以办多件事情，或一件事情的多个步骤可以一次办完，相信这一点已经毋庸置疑。而 `Cache line` 是从另一个侧面来看问题，它好比是在办事情时需要携带许多材料，有些时候因为缺少材料要反复回家拿。例如，我们去办证大厅办理证件，每种证件都会有很多个步骤、很多分材料，这些我们都可以预先准备好，一次搞定，而不用因为缺少材料回家拿再办理。  
  
即， `Cache line` 将“连续的一段内存区域”进行 `Cache` ，不是每次就 `Cache` 一个内存单元，而是一系列内存单元。在计算机中，通常以连续 64 字节为基本单元进行 `Cache` 操作。那它与我们的 `Java` 程序到底有什么关系呢？

循环遍历二维数组 `int[][] a = new int[5][10]` ，有两种方法：先循环外层，在循环内层；先循环内层在循环内层。第一种的效率高。
>```java
int a[][] = new int[2][100];
int b[][] = new int[100][2];
// 两者内存开销的区别在第三章详说。
```
  
这个结论和 `Cache line` 有什么关系呢？因为 `Java` 数组在内存中分配是先分配第一维，然后再分配多个第二维自数字，也就是说， `a[0][x]` 和 `a[1][x]` 是位于两个不同数组上的，空间也自然不会在一起。  
  
单个数组的内存空间是连续的，当获取 `a[0][0]` 时， `Cache line` 操作通常会将 `a[0][0]` 先关的一些数组元素（如 `a[0][1]` 、 `a[0][2]`......`a[0][7]` ）全部都 `Cache` 到 `CPU
` 的缓存中，当使用第一种遍历方式来遍历时，这些连续的数据都只需要 `Cache` 一次，就可以遍历完成。
>`Cache line` 的目的是为了快速访问，当你对内存修改时，回写内存中并不是按照行进行的。

## 2.4 缓存一致性协议
当有来自于内存中的同一份数据 `Cache` 在多个 `CPU` 中，且要求这些数据的读写一致时，多个 `CPU` 之间就需要遵循缓存共享的一致性原则。  
  
这种模型，在 `CPU` 上的实现就对应于多个 `CPU` 自己都会得到同一个内存单元的拷贝，当某个 `CPU` 修改它们的共享数据时，需要通知另一个 `CPU` 已经修改。  
  
内存单元有修改（`Modified`）、独占（`Exclusive`）、共享（`Shared`）、失效（`Invalid`）多种状态，多个 `CPU` 通过总线相互连接，每个 `CPU` 的 `Cache` 除了要去响应本身所对应的 `CPU` 的读写操作外，还需要监听总线上其他 `CPU` 的读写操作，通过监听对自己的 `Cache` 做相应的处理，形成了一种虚共享，这个协议叫作 `MESI` 协议。这个协议比较复杂， `Java` 程序员大概知道它有几个基本规则即可（一个 `CPU` 加载一块内存是专用的；当另一个 `CPU` 也加载了同一块内存时，那么此时两个 `CPU` 持有的信息将处于共享状态。两者的区别是， `CPU` 加载数据前是否监听到“读”广播；当发生写操作时会先尝试发起写广播，这个广播则会改掉数据，并且相应的其它 `CPU` 监听到数据被修改后，则会将自己的相应拷贝设置为失效状态）。  
  
一个数据修改了，它需要告诉其它的 `CPU` 这份数据被修改，现代 `CPU` 中 `Intel` 的微架构技术是 `QPI` 来完成的，其它的 `CPU` 会根据其 `CPU` 的架构不同而有不同的设计，不过 `CPU` 之间的交互时有一个时间差的，通常在 20ns~40ns 级别，频繁的交互会导致系统性能下降，而且 `CPU` 的使用量会飙高。换句话说，一个公司人多了，大家的工作都混乱，都还要交互开会，都在忙，其实没做多少事情。  
  
那么，有什么样的一些场景呢？例如，一些人设计一个数组来表示一组状态，数组中多个元素下标分别表示一种“状态”，每个线程独自享有自己的状态。这个设计表面上看好像已经脱离了锁：
```java
class VolatileInteger{
  volatile int number;
}
VolatileInteger[] values = new VolatileInteger[10];
for (int i = 0; i < 10; i++){
  values[i] = new VolatileInteger();
}
```
此时如果有多个 `CPU` 使用这个数组中的元素，即便每个“线程”只是操作其对应的某个指定的数组下标，那么数组中的多个元素由于 `Cache line` 也可能会被不同的 `CPU Cache` 到，每个 `CPU` 可能只修改到某个元素，但会有大量的交互存在，导致出现大量的 `QPI` 现象。
>疑惑： `Java` 的数组对象，数组中仅仅包含引用，对象应当有单独的空间，而不包含内容的怎么会有连续的空间呢？  
答曰： `Java` 的堆内存分配时，会相对较为连续地去分配，这样在一些问题的处理上会比较方便。上面的代码是使用 `for` 循环来分配空间的，分配过程很快，如果没有并发，其实是相对较为连续的。（大家要理解虚拟内存的概念，连续相对来说是虚拟内存，在物理内存并不是，这样可以更好的利用物理内存碎片）若发生了 `GC` ，同一个对象相关的引用合并后，它们的内存空间也会相对较为连续，所以 `Cache line` 的概率是极高的。

## 2.5 上下文切换
线程已经执行了一部分内容，需要记录下它的内容和状态，中途由于调度算法（例如分时间片、调用让步或休眠等）或阻塞等原因，导致还未完全结束的线程离开 `CPU` 的调度，此时需要保留现场，以便于在重新调用时找到它上一次执行到哪里了。最为常见的例子是在 `I/O` 方面，例如一次普通的网络 `API` 调用、一次数据库访问、一次磁盘请求（写一条没有缓冲的日志），此时线程会陷入 `Blocking` 状态，以至于发生所谓的“上下文切换”。  
  
大多数书籍中会提到进程比线程有更多的开销，进程独立向 `OS` 申请资源，而线程是共享进程的资源，其实就状态来讲，它们的区别并不是特别大，各有好处和坏处。
  
目前 `CPU` 调度的最基本单位是线程， `Java` 也是基于多线程模式的，而非多进程模式，因为资源共享，在 `Java` 中一个线程可以让一个静态的数据变大，将一个 `JVM` 进程直接挂死掉，这种情况在多进程模式终不会发生。  
  
通常在实际运行中会有代码段和数据段，内容切换时要保存这些运行中的上下文信息，需要使用的时候还要加载回来，以便于知道哪个位置继续开始执行，以及执行中的本地信息。  
  
类似于上面提到的日志写操作，现在很多开源的日志工具（如 `log4j` ）都采用异步模式去实现日志写操作，而程序通常不直接参与这个写操作，那么大部分线程就可以直接完成处理。它的实现方式是，日志写操作知识将日志写入一个消息队列中，由单独的线程来完成写操作，如果用阻塞的方式就会阻塞当前执行业务操作的这个线程。  
  
这种处理方式，类似于在银行排队时，很多人都需要等待办理业务，假如有一个 `ATM` 存款机，这样只要存款就去 `ATM` ，而不用与业务员说些与存款无关的事情，这样让排队的人能更快，只不过要多一个线程 `ATM` 处理存款业务。
>`ATM` 存款是否需要排队？这要根据紧急情况而定，例如某个人很急，需要立刻存款，我们也是可以理解的，当然让他查下队，反正我们不急。根据业务实际情况而定是否强制排队。

## 2.6 并发与征用
只要是服务端程序，迟早会遇到并发。当并发时，就会存在对各种资源的征用，包括对各个部件的征用，例如 `CPU` 。
>当程序去访问一些共享的内存区域时，并发问题同样是要考虑的，而这些共享区域并不仅仅局限于自己编写的代码，许多框架也提供了这些共享区域（例如连接池），当多个线程去征用这些数据的时候，都是用开销的。如果在某些特定的场景下需要自己来写这样的代码，你会思考这些问题吗？  
  
当程序中出现“加锁”时，说明这块区域是一块“临界区”，临界区的范围取决于加锁的对象是谁，以及跨越的代码段。如果多个线程尝试进入临界区，那么不论是单核还是多核的系统，也只允许一个线程进入这个区域，其余的通常会被放入一个等待队列中，进入 `Blocking` 状态。

通过上面内容的介绍，发现征用就像“独木桥”只允许一个人在上面玩，如果这个独木桥很长，那么系统就会被逐步串行化。  
  
征用带来更多的是同步的开销，它会付出许多指令要求在所有的 `CPU` 处理中不允许其它的线程进入临界区，而且需要将等待的线程放入阻塞队列，当前一个线程退出临界区以后还需要激活被阻塞的线程。  
  
这时我们希望这个独木桥变短一些，变成一截截的独木桥，这就有点像异步了，它将粒度锁定在很短的距离内（例如，当前一个人上桥 10 米后，后一个人就可以上桥了，而不需要等到前一个人过桥），在锁的这个大范围内，可以将一个大锁分解为多个小锁。好像还忽略了一个问题：如果允许一个人上桥，那么在等待上桥的许多人中，是谁上桥呢？此时要么有一个全局的管理调度者，要么就是大家去争抢上桥的机会。如果将桥分解成多截，那么每一截都有可能存在这样的征用情况，这个开销也是不能忽略的。（例如多线程效率和上下文切换消耗）。  
  
征用 `CPU` 的访问也不仅仅体现在锁上面， `CPU` 本身数量是有限的，即使没有锁，一个 `CPU` 也不会同时处理多个线程的任务，当有多个线程同时要求一个 `CPU` 来处理时， `CPU` 会对这些任务进行队列化， `CPU` 会基于时间片、优先级、任务大小等进行调度，这样就会产生“上下文切换”问题。  
  
**那么到底应该如何配置线程数呢？**
### 2.6.1 系统 `CPU` 密集度
一般系统分为计算密集型和 `I/O` 密集型两种。所谓计算密集型就是指系统大部分时间是在做程序正常的计算任务，例如数字运算、赋值、分配内存、内存拷贝、循环、查找、排序等，这些处理都需要 `CPU` 来完成，所以也叫 `CPU` 密集型。  
  
`I/O` 密集型是指系统大部分时间是在做 `I/O` 交互，而这个时间线程不会占用 `CPU` 来处理（但是通常会在系统中记录下这个线程正在等待 `I/O` ，以便 `I/O` 数据返回时，系统可以将其激活，被 `CPU` 再次调度）。换句话说，在这个时间范围内，可以由其它的线程来使用 `CPU` ，因此就可以多配置一些线程。（ `CPU` 通知 `DMA` 进行 `I/O` ）

### 2.6.2 关键程序的各项时间比例
1. 普通常规的操作。理论上我们认为这些操作都需要 `CPU` 调度来处理（不论是否需要将数据从主内存加载到栈或者加载到 `CPU` 的 `Cache` 中，总之需要 `CPU` 来处理），在最理想的情况下，建议将线程数设置为 `CPU ± 1` ，这样理论上是没有上下文切换的。
2. `I/O` 操作。例如，系统中一段比较“关键”的程序访问总共花费了 120ms ， `I/O` 操作占用了 100ms ，那么意味着在这 100ms 时间内 `CPU` 是可以被其它线程访问的。此时，这个程序在单核系统中的线程数理论上可以设置为 6 ，在多核系统中自然是乘以 `CPU` 的个数，一般会在这个数字上下浮动。（一切要以实际场景为主）
3. “锁”，也就是临界区的范围。前面提到了它有粒度，在这个区域内不论有多少个 `CPU` ，也无法同时进入。因此这部分不好计算。首先看这个锁的对象是不是静态对象或 `Class` ，如果是，则可以认为是一个 `JVM` 进程全局锁，那么无论配置多少个线程都是一样的，并且这个时候不能乘以 `CPU` 的个数，因为锁是全局的。（例如： `ConcurrentHashMap` 内部默认分解为 16 个 `Segment` ，数据都是先查找 `Segemnt` ， 再在内部加锁，因此在理想情况下，锁粒度可以降低 16 倍，那么自然的应该允许 16 个并行。当然并不排除由于热点问题，导致某些 `Segment` 上的请求更多）。（所谓公式也只是告知底层原理的另一种方法，但是实际情况往往是千变万化的，很多时候我们甚至希望系统有一些自适应能力，根据实际情况自动来做一些调节）。

### 2.6.3 `JVM` 本身的调节
不论 `CPU` 跑得多么快，如果 `JVM` 在不断地做 `GC` 操作，或者因为其它因素跑得慢，那么怎么配置线程池，系统的性能也不可能上来。  
  
很多时候，我们不希望程序在磁盘（ `Swap` ）上跑，除非是 `SSD` 硬盘。有些时候，宁愿系统宕机，也不愿意让程序跑得很慢，因为磁盘上跑的程序在不断发生 `I/O` 操作，我们认为程序很正常，但是外部运行却奇慢无比，而又找不到原因。  
  
可以根据 `JVM` 运行日志中，平均做 `Young GC` 的时间间隔，以及系统统计的 `QPS` （请求数/秒），来估算每个请求大致占用的内存大小，虽然不十分准，但是有参考价值。
>`Eden` 空间的大小我们是知道的，通常一个请求分配的空间都在 `Eden` 区域， `Eden` 区域满的时候就会发生 `Young GC` 。所以有了 `Young GC` 的时间间隔，就可以知道多长时间会填充满 `Eden` 区域，例如 3s ，进一步通过 `QPS * 3` 得到多少个请求可以填充满 `Eden` 区域，那么自然的可以计算出平均每个请求的内存空阿金大小。
  
  
  
[1]: http://blog.csdn.net/phunxm/article/details/8952963
